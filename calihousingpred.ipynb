{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd   \n",
    "import matplotlib_inline \n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np     \n",
    "df = pd.read_csv('housing.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_set, test_set = train_test_split(df, test_size=0.2, random_state=42)   #splitting into train and test set while keeping the records of the sets same\n",
    "#print(len(train_set)) \n",
    "#print(len(test_set))     \n",
    "df[\"income_cat\"] = pd.cut(df[\"median_income\"], bins=[0., 1.5, 3.0, 4.5, 6., np.inf], labels=[1, 2, 3, 4, 5])   \n",
    "#pd.cut is used to set data values into bins. df[\"median_income\"] column is the input data for categorization.      \n",
    "#bins=[0., 1.5, 3.0, 4.5, 6., np.inf] specifies the bin edges. Each bin includes the left edge but excludes the right edge, except for the last bin which includes both edges.    \n",
    "#labels=[1, 2, 3, 4, 5] assigns these labels to the bins.\n",
    "#df[\"income_cat\"].hist() \n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit                   #used for making stratified test and training set\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)   #split is the object for stratified splitting\n",
    "for train_index, test_index in split.split(df, df[\"income_cat\"]):            #loop for splitting the data into stratified training set and stratified test set  \n",
    "    strat_train_set = df.loc[train_index]\n",
    "    strat_test_set = df.loc[test_index] \n",
    "#print(strat_test_set[\"income_cat\"].value_counts())  \n",
    "strat_train_set = strat_train_set.drop(\"income_cat\", axis=1)                 #used to drop the income cat\n",
    "strat_test_set = strat_test_set.drop(\"income_cat\", axis=1) \n",
    "df=strat_train_set.copy()                                                    #used to copy the training set\n",
    "df.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4, s=df[\"population\"]/100, label=\"population\", c=df[\"median_house_value\"], cmap=plt.get_cmap(\"jet\"), colorbar=True)              #used to create a scatterplot showing its population and median house value\n",
    "plt.legend() \n",
    "#plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numeric=df.select_dtypes(include=[np.number])                             #drops non numeric columns\n",
    "corr_matrix=df_numeric.corr()                                                #a matrix to find correlation\n",
    "#print(corr_matrix[\"median_house_value\"].sort_values(ascending=False))           #finds correlation b/w median house value and other attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(kind=\"scatter\", x=\"median_income\", y=\"median_house_value\", alpha=0.8) \n",
    "#plt.show()  \n",
    "df=strat_train_set.drop(\"median_house_value\", axis=1)                           #removes the median_house_value column from strat_train_set, leaving only the feature variables. Now df conatins all features except median house value\n",
    "df_labels=strat_train_set[\"median_house_value\"].copy()                          #creates a separate variable (df_labels) containing only the target variable (median_house_value).   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer \n",
    "imputer=SimpleImputer(strategy=\"median\")                                     #creating the simpleimputer instance\n",
    "df_num=df.drop(\"ocean_proximity\", axis=1) \n",
    "imputer.fit(df_num)                                                          #fills the missing values with median values in the entire dataset\n",
    "#print(imputer.statistics_)   \n",
    "x=imputer.transform(df_num)                                                  #using the trained imputer to transform the dataset by filling the missing values\n",
    "df_tr=pd.DataFrame(x, columns=df_num.columns, index=df_num.index)            #transforms the array back to dataframe form \n",
    "df_cat=df[[\"ocean_proximity\"]] \n",
    "#print(df_cat.head(10))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder \n",
    "og_en=OrdinalEncoder()                                                       #encoder object\n",
    "df_cat_en=og_en.fit_transform(df_cat)                                        #Fits the encoder to the ocean_proximity values and transforms them into integers based on lexicographical order.\n",
    "#print(df_cat_en[:10]) \n",
    "#print(og_en.categories_)   \n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder \n",
    "cat_en= OneHotEncoder()                                                      #onehotencoder object \n",
    "df_cat_1hot= cat_en.fit_transform(df_cat)                                    #fits the onehotencoder to df_cat and transforms it\n",
    "#print(df_cat_1hot.toarray())                                                 #gives the output as dense numpy array instead of scipy sparse matrix \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator , TransformerMixin  \n",
    "rooms_ix , bedrooms_ix, pop_ix, hhlds_ix = 3,4,5,6                           #indices of columns\n",
    "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):              #importing base estimator and transformermixing class\n",
    "    def __init__(self, add_bedrooms_per_room= True):                         #costructor to add bedrooms per room and decides whether to calculate add bedrooms per room feature or not\n",
    "        self.add_bedrooms_per_room = add_bedrooms_per_room \n",
    "    def fit(self, X, y=None):                                                #doesnt do anything but is needed to comply with scikit learn api\n",
    "        return self \n",
    "    def transform(self, X):                                                  #function to take input x and create new features\n",
    "        rooms_per_hhld = X[:, rooms_ix] / X[:, hhlds_ix] \n",
    "        pop_per_hhld = X[:, pop_ix] / X[:, hhlds_ix] \n",
    "        if self.add_bedrooms_per_room:                                               #if add bedrooms per room feature is true, then calculate bedrooms per room\n",
    "            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix] \n",
    "            return np.c_[X, rooms_per_hhld, pop_per_hhld, bedrooms_per_room] \n",
    "        else: \n",
    "            return np.c_[X, rooms_per_hhld, pop_per_hhld] \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline                                        \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "num_pipeline = Pipeline( [ ('imputer', SimpleImputer(strategy=\"median\")), ('att_adder', CombinedAttributesAdder()), ('std_scaler', StandardScaler()) ]) \n",
    "df_num_tr=num_pipeline.fit_transform(df_num)                                 #transformer pipeline created \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer \n",
    "num_att=list(df_num) \n",
    "cat_att=[\"ocean_proximity\"] \n",
    "full_pipeline=ColumnTransformer( [ (\"num\", num_pipeline, num_att), (\"cat\", OneHotEncoder(), cat_att) ] ) \n",
    "df_prepared=full_pipeline.fit_transform(df)                                  #applying the pipeline to complete data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor                                                          #using xgbregressor\n",
    "xbg_reg=XGBRegressor(random_state=42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV                                                    #to search for best parameter combinations for the model\n",
    "param_grid=[ { 'n_estimators': [50,100,150], 'max_depth': [3,5,7], 'learning_rate': [0.01, 0.05, 0.1], 'subsample': [0.7, 0.9, 1.0], 'colsample_bytree': [0.7, 0.9, 1.0] } ]  \n",
    "grid_search=GridSearchCV(xbg_reg, param_grid, cv=5, scoring='neg_mean_squared_error', verbose=2, n_jobs=-1)                #verbose controls how much information gets printed while the model is training, n_jobs controls how any cpu cores will be used\n",
    "grid_search.fit(df_prepared, df_labels) \n",
    "#print(grid_search.best_estimator_) \n",
    "#print(np.sqrt(-grid_search.best_score_))  \n",
    "\n",
    "final_model = grid_search.best_estimator_                                                      #it saves the model with best hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score                                       #training the model\n",
    "xgb_pred=final_model.predict(df_prepared) \n",
    "xgb_mse=mean_squared_error(df_labels, xgb_pred) \n",
    "xgb_rmse=np.sqrt(xgb_mse) \n",
    "xgb_r2=r2_score(df_labels, xgb_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(xgb_rmse)   \n",
    "#print(xgb_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=strat_test_set.drop(\"median_house_value\", axis=1) \n",
    "y_test=strat_test_set[\"median_house_value\"].copy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_prepared=full_pipeline.transform(X_test)                                       #test set prepared\n",
    "final_pred=final_model.predict(X_test_prepared) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_mse=mean_squared_error(y_test, final_pred)                                      #checking for accuracy\n",
    "final_rmse=np.sqrt(final_mse) \n",
    "final_r2=r2_score(y_test, final_pred)\n",
    "print(\"accuracy: \", final_r2)                                                         #85.24%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
